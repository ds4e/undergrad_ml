{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a9798f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Selection\n",
    "### `! git clone https://github.com/ds4e/model_selection`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7aeff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "- Why and how do we build models?\n",
    "- There are three key pieces:\n",
    "    1. The variables in the raw data\n",
    "    2. Cleaned variables and transformations of them (the feature space)\n",
    "    3. The model class (hyperparameter choices: variable selection, \"$k$\")\n",
    "- How do we \"practice\" machine learning? What are the principles of good model-building?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f561311b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Feature Engineering\n",
    "2. The Bias-Variance Trade-Off\n",
    "3. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad8d31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c6c00d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Spaces\n",
    "- The data that we get is just raw material: We have to deliberately create a **feature space** that allows models to be expressive and clever about using the data\n",
    "- Already, we've seen various useful ways of processing the data: Log/IHS transformations, maxmin scaling, one-hot encoding\n",
    "- When we create/transform new variables, we create new information that was previously unavailable, expanding the space of models that we can estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c9d9f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "- Pick a dataset with a numeric outcome and some promising variables/features\n",
    "- Discuss how you think those variables determine the target/outcome, in general\n",
    "- We're then going to talk about strategies to build a feature space in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d48a2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We've already seen...\n",
    "- Our most useful transformation is probably the natural logarithm (or inverse hyperbolic since when there are zeros or negative values): Converts variables with long tails into more bell-shaped ones\n",
    "- One Hot Encoding: Transform one categorical/qualitative variable with $K$ labels into $K$ 0/1 variables that jointly encode the original label (remember to drop one label to avoid the dummy variable trap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd1bb23",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normalization/Standardization\n",
    "- Especially with neighbor-based models and neural networks, normalizing the variables is important for computational stability\n",
    "- Maxmin: \n",
    "$$\n",
    "u_i = \\frac{x_i - \\min(x)}{\\max(x)-\\min(x)}\n",
    "$$\n",
    "- $z$-score: If all the variables are roughly bell-shaped (or are, after a log/ihs transformation), we can center them around 0 and give them a standard deviation of 1,\n",
    "$$\n",
    "z_i = \\frac{x_i - \\bar{x} }{s(x)}\n",
    "$$\n",
    "- Robust scaling: If there are outliers that make $z$-scaling unreliable,\n",
    "$$\n",
    "r_i = \\frac{x_i - \\text{median}(x)}{IQR(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c1fdb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Missing Value Dummies\n",
    "- For regression models, the following approach works very well:\n",
    "\n",
    "1. Create a missing value dummy, `df['var_na'] = df['var'].isna()`\n",
    "2. Replace missings with zero, `df['var_0'] = df['var'].nafill(0)`\n",
    "3. Including both `var_na` and `var_0` in any regression\n",
    "\n",
    "Why does this work?\n",
    "\n",
    "$$\n",
    "... + b_{\\text{var}_{NA}} \\text{var}_{NA} + b_{\\text{var}_0} \\text{var}_0+... = \\begin{cases}\n",
    "\\beta_{\\text{var}_0} \\text{var}_0, & \\text{ data available }\\\\\n",
    "\\beta_{\\text{var}_{NA}}, & \\text{ data missing }\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- So the model smoothly switches back and forth between a linear model when data are available, and a missing data dummy when it's not available\n",
    "- **Only do this with models based on linear combinations of variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab851a6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Polynomial Expansion\n",
    "- In math, there's a fundamental concept (Stone-Weierstrass Theorem), that can be summarized like this: Any continuous function $\\mu(x)$ can be approximated arbitrarily well by polynomial functions of the form\n",
    "\\begin{alignat*}{2}\n",
    "m(x,b) &=& \\underbrace{b_0}_{\\text{0-order, constant, scalar}} + \\underbrace{\\sum_{\\ell = 1}^L b_\\ell x_\\ell}_{\\text{First-order, linear, vector}} \\\\\n",
    "&& + \\underbrace{\\sum_{\\ell =1}^L \\sum_{j=1}^L b_{\\ell j} x_\\ell x_j}_{\\text{Second-order, quadratic, matrix}} + \\underbrace{\\sum_{\\ell =1}^L \\sum_{j=1}^L \\sum_{p=1}^L b_{\\ell j p} x_\\ell x_j x_p}_{\\text{Third-order, quartic, tensor}} + \\underbrace{...}_{\\text{Higher order terms}} ,\n",
    "\\end{alignat*}\n",
    "in the sense that $\\max_{x} \\min_{b} | \\mu(x) - m(x,b) |$ can be made as small as desired by adding more higher order terms and optimizing over $b$\n",
    "- This motivates our interest in **linear models**: We can approximate any **known** function $\\mu(x)$ arbitrarily well with a model $m(x,b)$ where the $b$ coefficients are multiplicative with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68260572",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Polynomial Expansion in Scikit\n",
    "\n",
    "1. **Import Expander**: `from sklearn.preprocessing import PolynomialFeatures`\n",
    "2. **Create Expander**: `expander = PolynomialFeatures(degree=2,include_bias=False) # Create the expander`\n",
    "3. **Fit Expander**: `X_poly = expander.fit_transform(X) `\n",
    "4. **Store variable names**: `poly_manes = expander.get_feature_names_out() `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5a3a2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Interaction Terms\n",
    "- Among the terms of the polynomial expansion we are most interested in, are terms of the type\n",
    "$$\n",
    "b_{jk} x_{j}x_{k}, \\quad j \\neq k\n",
    "$$\n",
    "or\n",
    "$$\n",
    "b_{jk\\ell} x_{j}x_{k}x_{\\ell}, \\quad j \\neq k \\neq \\ell\n",
    "$$\n",
    "which capture the joint non-linear impact of changes in $x_j$ and $x_k$ or $x_\\ell$ together. \n",
    "- For example, the difference in price between a BMW with a sunroof versus without might be larger than the difference in price between a Honda with a sunfroof versus without. The BMW might exhibit more \"luxury bang-for-buck\" effects than more budget-conscious vehicles.\n",
    "- Use `interaction_only=True` in Scikit's polynomial expander to get only the interactions, if you want to avoid high order powers of the variables alone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f7fbc7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Temporal Features/Controls (Optional)\n",
    "- Some features/controls are periodic or recurrent, and a linear model seems like a bad fit: Think about hours of the day\n",
    "- In cases like this, we can do two things\n",
    "\n",
    "1. Bin the time variable into a categorical and one-hot encode (e.g. each hour of the day gets its own coefficient in the regression)\n",
    "2. Use a Fourier Series:\n",
    "$$\n",
    "m(t,b) = b_0 + \\sum_{\\ell=1}^L \\left[ b_{\\ell,\\cos} \\cos \\left(2 \\pi \\ell \\dfrac{ t}{T} \\right) + b_{\\ell,\\sin} \\sin \\left( 2 \\pi \\ell \\dfrac{t}{T} \\right) \\right],\n",
    "$$\n",
    "where $T$ is the period length ($T=24$ for hours, $T=7$ for day-of-week, $T=365$ for day-of-year, etc.)\n",
    "\n",
    "The Fourier Series is still a linear model in the $b$ coefficients, just like the polynomial family, but it's built from sines and cosines, so it behaves periodically and continuously as $t/T$ ranges over its possible values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e451fc1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fourier Series Basis Functions (Optional)\n",
    "- These are like polynomial families $1, x, x^2, x^3,...$, but for variation over time:\n",
    "<img src=\"./src/fourier.png\" alt=\"Illustration of bias and variance.\" width=\"100%\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617f5d8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Time Series: Temporal Target/Outcome (Optional)\n",
    "- Imagine your target variable is denominated in time, so it's the $y_{it}$ rather than $x_{it}$ that is temporal\n",
    "- Instead of studying $y_{it}$ directly, take the **first difference**\n",
    "$$\n",
    "z_t = y_t - y_{t-1}\n",
    "$$\n",
    "or compute the **log growth** or **log return** (particularly in fenance)\n",
    "$$\n",
    "r_t = \\log \\left( \\frac{ y_t }{ y_{t-1} } \\right)\n",
    "$$\n",
    "- In regression, we often include **lags** of the variable to account for its \"momentum\" or \"trajectory\" over time, like\n",
    "$$\n",
    "\\hat{z}_t = b_0 + b_1 z_{t-1} + b_2 z_{t-2} + ... + b_K z_{t-K}\n",
    "$$\n",
    "So we model the variable's evolution over time based on recent changes in its values, rather than levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0dd177",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "- Engineer a feature space for your data\n",
    "- Do a train-test split\n",
    "- Train a simple linear model on the training data\n",
    "- Train a complex linear model on the training data\n",
    "- Which does better (lower MSE) on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a3a991",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. The Bias-Variance Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c8b6c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Machine Learning\n",
    "- Many fields are driven by a handful of core ideas\n",
    "- For machine learning/data science, one of these core ideas is called the **bias-variance trade-off**\n",
    "- This is a mathematical concept and we'll briskly cover the quantitative argument behind it, and then focus on the intuition it provides\n",
    "- This discussion is mathematical, to show how you discover and think about an idea like this\n",
    "- This lesson is essentially the \"general theory of machine learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49fdad5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expectation\n",
    "- One of our favorite quantities is the sample mean of a variable $X$,\n",
    "$$\n",
    "m(X) = \\sum_{i=1}^N \\frac{1}{n} x_i\n",
    "$$\n",
    "- We want to imagine a similar concept: What value of $X$ is most likely, before we've observed the data?\n",
    "- We are going to replace $1/n$ with the abstract probability that $X=x$ occurs, $p(x)$, and sum over possible values of $X$\n",
    "- The **expectation of a random variable $X$** is\n",
    "$$\n",
    "\\mathbb{E}_X[X] = \\sum_{\\text{All } x \\text{ such that }p(x)>0} p(x) x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e82eee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expectation\n",
    "- Another way to understand this: Imagine computing $m(X)$ by first computing the histogram, and weighting each $x_i$ by the height of the histogram\n",
    "- Another way to understand this: It answers the question, \"Before we actually gather any data, what do you expect the sample mean to be?\"\n",
    "- As data scientists we love this: It's like we're thinking about the model before we actually gather data or do any analysis\n",
    "- Key observation: $\\mathbb{E}_X[X]$ is just a number, like 7.46. So $\\mathbb{E}_X[ \\mathbb{E}_X[X] ] = \\mathbb{E}_X[X]$ (idempotent operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e493a60",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expectation: Example\n",
    "- What is the expected value of rolling a single die?\n",
    "- What is the expected value of rolling two dice and summing the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fd878",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias and Model Variance\n",
    "- There are two properties of an estimate that it's important to keep in mind:\n",
    "\n",
    "1. **Bias**: If the truth is $T$ and we predicted $\\hat{T}$, the bias is equal to $\\mathbb{E}_{\\hat{T}}[\\hat{T} - T]$, the expected difference between the true value and out predicted value\n",
    "2. **Variance**: The variability of our prediction is $\\mathbb{E}_{\\hat{T}}[ (\\hat{T}-T)^2 ]$\n",
    "\n",
    "We obviously don't like bias: If it is large, we make lots of mistakes in expectation. But it turns out that we also don't like variance: If the variance is large, our predictions are typically unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb41f6a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias and Variance: Classic Picture\n",
    "\n",
    "<img src=\"./src/bvt.png\" alt=\"Illustration of bias and variance.\" width=\"65%\" height=\"auto\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c138e37",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias-Variance Trade-Off\n",
    "- It turns out that bias and variance are the core considerations of how we build models\n",
    "- There's a bunch of features/covariates $x_1, x_2, ..., x_L$, which jointly determine the expected value of $y$, which is $\\mathbb{E}_X[y|X] = \\mu(x_1, x_2, ..., x_L)$\n",
    "- However, there's an additive shock, $\\varepsilon$, with mean zero, so we never observe $\\mu$ directly:\n",
    "$$\n",
    "y(x_1, x_2, ..., x_L) = \\mu(x_1, x_2, ..., x_L) + \\varepsilon\n",
    "$$\n",
    "- We assume the expected value of $\\varepsilon$ is $\\mathbb{E}[\\varepsilon] = 0$ and its variance is $\\mathbb{E}[\\varepsilon^2] = \\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f481465",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias-Variance Trade-Off\n",
    "- Given this set-up, what is machine learning?\n",
    "- What we're doing is guessing $\\mu(x_1, x_2, ..., x_L)$ with a model $m(x_1, x_2, ..., x_L; b)$, and then choosing $b$ to solve this:\n",
    "$$\n",
    "\\min_{b} \\frac{1}{n} \\sum_{i=1}^n (\\mu(x_i) + \\varepsilon_i - m(x_i,b))^2\n",
    "$$\n",
    "- Our model, $m(x,b)$ is just a model: $k$-NN, linear, whatever we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8c5cb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expected Loss\n",
    "- **Before** we gather the data, what do we **expect**ed to happen with our loss function (MSE)?\n",
    "\\begin{alignat*}{2}\n",
    "\\mathbb{E}_{X,\\varepsilon}\\left[ \\frac{1}{n} \\sum_{i=1}^n (\\mu(x_i) + \\varepsilon_i - m(x_i,b))^2 \\right] \n",
    "\\end{alignat*}\n",
    "- Let's work out how to simplify this There's essentially two tricks: FOIL and simplify, then add/subtract and FOIL again, using the expected value to erase a surprising number of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15445988",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias-Variance Trade-Off\n",
    "- So our MSE breaks into three distinct pieces:\n",
    "\\begin{alignat*}{2}\n",
    "\\mathbb{E}_{X,\\varepsilon}\\left[ \\text{MSE}(b) \\right] &=& \\underbrace{\\mathbb{E}_{X,\\varepsilon}\\left[ \\frac{1}{n} \\sum_{i=1}^n  (\\mu(x_i) - \\mathbb{E}_{X}[m(X,b)])^2 \\right]}_{\\text{Predictor Bias}} + \\underbrace{\\mathbb{E}_{X,\\varepsilon}\\left[ \\frac{1}{n} \\sum_{i=1}^n (\\mathbb{E}_{X}[m(X,b)]- m(x_i,b))^2 \\right]}_{\\text{Model Variance}}\\\\\n",
    "&& + \\underbrace{\\sigma^2}_{\\text{Irreducible Error}}\n",
    "\\end{alignat*}\n",
    "and the take-away intuition from this equation is that\n",
    "$$\n",
    "\\text{Expected Loss} = \\text{Bias-Squared} + \\text{Model Variance} + \\text{Irreducible Error}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c2ed5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias-Variance Trade-Off\n",
    "\n",
    "- So our expected loss decomposes into three pieces:\n",
    "    1. **Predictor Bias**: In expectation, how far do we expect our model $m$ to be from the true $\\mu$?\n",
    "    2. **Model Variance**: Regardless of the truth, how variable is our model in expectation?\n",
    "    3. **Irreducible Error**: How much inherent noise is there in the environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e761927",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## BVT for $k$-NN\n",
    "- For the $k$-neighbors regressor model, there's an explicit formula:\n",
    "$$\n",
    "\\mathbb{E}_{X,y}[(y-m(x))^2|X=x] = \\underbrace{\\left( \\mu(x) - \\frac{1}{k}\\sum_{\\text{$i$ in Neighbors}(x)}^K y_i \\right)^2}_{\\text{Bias squared}} + \\underbrace{\\frac{\\sigma^2}{k}}_{\\text{Variance}} + \\underbrace{\\sigma^2}_{\\text{Irreducible Error}}\n",
    "$$\n",
    "- As $k$ increases, the model variance decreases: You are averaging over more and more neighbors, so the predictor variance shrinks\n",
    "- As $k$ increases, the bias squared increases: You are averaging over cases that are further and further from $x$, so in expectation, they are becoming less relevant\n",
    "- This is the BVT in practice: A very low $k$ and a very high $k$ are typically both not the solution, and the optimal $k$ depends on the prediction environment in question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656eb94d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimal Model Complexity\n",
    "\n",
    "<img src=\"./src/model_complexity.png\" alt=\"Illustration of model complexity trade-off.\" width=\"75%\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a372c165",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Over-fitting, Under-fitting\n",
    "- Excessively complex models are associated with high variance but low bias, leading to high expected loss: This is called **over-fitting**\n",
    "- Excessively simplistic models are associated with low variance but high bias, leading to high expected loss: This is called **under-fitting**\n",
    "- We want to balance **parsimony** with **verisimilitude**, and avoid both under- and over-fitting\n",
    "- This is what the train-test split is about: Using data to judge when the model is transitioning from simple and robust to overly complicted and unreliable\n",
    "- This requires judgment and appropriate use of model selection tools\n",
    "- This is a fundamental concept that we'll talk about for the rest of class, and the kind of idea you should commit to your data science core archive of ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed89b40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681510f5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model Selection and Validation\n",
    "- Ok, we can build massive and complex feature spaces...\n",
    "- ... but we know this will typically lead to overfitting and needlessly complex models that aren't BVT-optimal\n",
    "- How do we estimate expected performance, so we can address concerns about the bias-variance trade-off?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b07b7c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "- For your data, pick a model selection/hyperparameter choice about which you are uncertain\n",
    "- For example, \"How many powers of $x$ should I include?\", \"How many lags of the time series should I use?\", \"Should I include this variable or not?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579a017",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $K$-Fold Cross Validation\n",
    "- This is a kind of \"$K$-way train/test split\":\n",
    "\n",
    "0. Fix the model (the variables, the neighbors, how many polynomial features, etc.)\n",
    "1. Partition the data into $K$ equally-sized sets (the folds)\n",
    "2. For each $k=1,...,K$, estimate your model's parameters on the complementary chunks, and test performance on the the $k$-th chunk\n",
    "3. Save the model's performance for each chunk (e.g. accuracy, MSE)\n",
    "\n",
    "This provides $K$ estimates of the model's performance, estimating the distribution function of the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876ddf9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"./src/crossvalidation.png\" alt=\"Illustration of $k$-Fold Cross Validation.\" width=\"80%\" height=\"auto\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f54ce0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scikit Implementation: Mostly Automatic\n",
    "\n",
    "``` python \n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "[...]\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=100) # Create folds\n",
    "\n",
    "scores = cross_val_score( # Conduct kfcv:\n",
    "    model,X,y, # Model and data\n",
    "    cv=kfold, # Folds\n",
    "    scoring=mean_squared_error # Loss function\n",
    ")\n",
    "\n",
    "print(\"Fold scores:\", scores)\n",
    "print(\"Mean score:\", np.mean(scores))\n",
    "print(\"Median score:\", np.median(scores))\n",
    "print(\"Std dev:\", np.std(scores))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c298c28",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Scikit Implementation: More Control\n",
    "\n",
    "``` python \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "[...]\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X[train_idx], X[val_idx]\n",
    "    y_train, y_test = y[train_idx], y[val_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat = model.predict(X_test)\n",
    "    score = mean_squared_error(y_test, y_hat)\n",
    "    scores.append(score)\n",
    "\n",
    "print(\"Fold scores:\", scores)\n",
    "print(\"Mean score:\", np.mean(scores))\n",
    "print(\"Median score:\", np.median(scores))\n",
    "print(\"Std dev:\", np.std(scores))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f3ef9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "- For your model selection question, use $k$-FCV to estimate the model's performance for a variety of plausible alternative, and pick the alternative with the best median or average performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d778c8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "- This was a lot! \n",
    "- Feature engineering is essential to build realistic, expressive models\n",
    "- But we can go too far, and end up over-fitting, or not far enough, and under-fit: this is the essence of the BVT\n",
    "- Cross validation allows us to quantify whether we're doing a good job or not\n",
    "- These are fundamental topics in the field of ML/DS\n",
    "- The connections with probability and simulation can be further developed, but getting some experience with the ideas first is a good idea\n",
    "- While we're here: The point of deep learning is to use a neural network to automatically do feature engineering for us (representation learning)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
