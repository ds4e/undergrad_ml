{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274671d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploratory Data Analysis\n",
    "### Foundations of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30297c83",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## EDA is Powerful\n",
    "- It's the early 2010's and Anne Case and Angus Deaton are doing research on mortality at Princeton\n",
    "- They get the CDC WONDER data and analogous international sources, and plot mortality. They see this:\n",
    "\n",
    "\n",
    "\n",
    "<img\n",
    "  src=\"./src/all_cause_mortality.png\"\n",
    "  style=\"width:45%; max-width:none;\"\n",
    "  alt=\"All cause mortality, 2000-2015.\"\n",
    "/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a78df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## EDA\n",
    "- Case and Deaton -- having determined that this is a real phenomenon and not a mistake -- dig into the mortality data by cause:\n",
    "\n",
    "<img\n",
    "  src=\"./src/mortality_by_cause.png\"\n",
    "  style=\"width:50%; max-width:none;\"\n",
    "  alt=\"Mortality by cause, 2000-2015.\"\n",
    "/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c733182",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## EDA\n",
    "- What is this? What have they discovered?\n",
    "- This is one of the most important trends in the last 25 years, and it was discovered because some people were playing with data and making very simple visualizations --- this is kind of shocking, perhaps even disturbing\n",
    "- The second plot is very clever: For example, Lung Cancer and Diabetes are included as counterpoints to the other causes, to show that some are declining are flat, and they are dashed instead of solid to set off that distinction\n",
    "- You cannot overstate the value of a well-constructed visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b47b96",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "- Today we look at some tools for exploratory data analysis, or EDA: How to visualze one or two variables, and basic statistics\n",
    "- Our goal is for you to be able to analyze one or more variables, and come to some basic conclusions about where the variation in the data is\n",
    "- Some good datasets for today are:\n",
    "    1. `nhanes_data_17_18.csv`\n",
    "    2. `airbnb_NYC.csv`\n",
    "    3. `craigslist_cville_Cars_long.csv`\n",
    "- We'll plot in both Pandas and Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74afdc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Numerical analysis\n",
    "import matplotlib.pyplot as plt # Basic plotting\n",
    "import seaborn as sns # Advanced plotting\n",
    "import pandas as pd # Dataframes and basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f5f5b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Analyzing a single variable\n",
    "2. Analyzing two variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209b4f2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analyzing a Single Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae983c9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Histogram\n",
    "- The classic way of visualizing the relative frequency with which a variable takes particular values is the **histogram**:\n",
    "    1. Group similar observations into $B$ distinct **bins**:\n",
    "        - For a categorical use the original class labels, or consolidate them until you have fewer than $B$ bins\n",
    "        - For a numerical variable, find the maximum and minimum, and set the bin size equal to $\\Delta = (x_{max}-x_{min})/B$. Then the $k$-th bin is $[x_{min} + (k-1) \\Delta, x_{min} + k \\Delta)$.  \n",
    "    2. Make a bar graph where the height of the $k$-th bin is proportional to the number of observations taking that range of values\n",
    "\n",
    "| Method | Usage |\n",
    "| :---: | :---:|\n",
    "| `df[var].plot.hist()` | Pandas histogram |\n",
    "| `sns.histplot(df[var])` | Seaborn histogram |\n",
    "| `sns.histplot(data=df,x=var)` | Seaborn histogram |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11249e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Long Tails\n",
    "- Many variables have a distinct pattern: A huge number of small values clustered in a handful of bars, and then many bars with almost no observations in them\n",
    "- This is a **right-skewed variable** or a variable with a **heavy tail**\n",
    "- Neither our eyes nor computers like these\n",
    "\n",
    "| Method | Usage |\n",
    "| :---: | :---:|\n",
    "| `df[var_ln] = np.log( df[var] )` | Natural logarithm, if there are no zero values |\n",
    "| `df[var_ihs] = np.arcsinh( df[var] )` | Inverse hyperbolic sine, if there are zero values |\n",
    "\n",
    "\n",
    "- Let's make a plot of log and inverse hyperbolic sine, to see how they work, then look at some examples in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea2ccbe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Statistics\n",
    "\n",
    "- The histogram is great\n",
    "- The histogram is complicated\n",
    "- To summarize variables, we often use **statistics** to capture information about the histogram in a handful of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439b2ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Statistics\n",
    "- A **sample** is a set of values drawn for some variable. \n",
    "- We usually denote a **sample of size $N$** as a list/vector, $X = [ x_1, x_2, ..., x_N]$\n",
    "- The **sample mean** is a measure of central tendency of a variable $X$:\n",
    "$$\n",
    "\\bar{x} = \\frac{x_1 + x_2 + ... + x_N}{N} = \\frac{1}{N} \\sum_{i=1}^N x_i\n",
    "$$\n",
    "- The **sample variance** is a measure of variation of a variable $X$:\n",
    "$$\n",
    "\\bar{s}^2 = \\frac{(x_1-\\bar{x})^2 + (x_2-\\bar{x})^2 + ... + (x_N-\\bar{x})^2}{N} =  \\dfrac{1}{N} \\sum_{i=1}^N (x_i - \\bar{x})^2\n",
    "$$\n",
    "- The **sample standard deviation** is a measure of variation of a variable $X$:\n",
    "$$\n",
    "\\bar{s} = \\sqrt{ \\dfrac{1}{N} \\sum_{i=1}^N (x_i - \\bar{x})^2 }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc171e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Statistics\n",
    "\n",
    "- Of course, we can use Pandas to compute these:\n",
    "\n",
    "| Method | Usage |\n",
    "| :---: | :---:|\n",
    "| `df[var].describe()` | Standard summary |\n",
    "| `df[var].mean()` | Mean |\n",
    "| `df[var].var()` | Variance |\n",
    "| `df[var].std()` | Standard Deviation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18482e8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kernel Density Plots\n",
    "- The classic histogram has some flaws; namely, the number of bins is not easy to select, and the apparent results can vary depending on the number of bins\n",
    "- The alternative is to fix a particular $x_0$, weight the points around it by distance, and average; this is called a **kernel density estimator (kde)**\n",
    "\n",
    "| Method | Usage |\n",
    "| :---: | :---:|\n",
    "| `df[var].plot.kde()` | Pandas KDE plot |\n",
    "| `sns.kdeplot(df[var])` | Seaborn KDE plot |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f455e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Always, always, always, look at your data\n",
    "\n",
    "<img\n",
    "  src=\"./src/lawyerSalaries2018.jpg\"\n",
    "  style=\"width:70%; max-width:none;\"\n",
    "  alt=\"Histogram of entry-level lawyer salaries.\"\n",
    "/>\n",
    "\n",
    "- How *useful* is it to say, \"The average yearly salary of a lawyer is about $100k?\"\n",
    "- Statistics can be incredibly misleading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbc46b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analyzing Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07067a51",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conditioning Categorically\n",
    "\n",
    "- We often want to understand how one variable $Y$ behaves as another variable $X$ is varied\n",
    "- If $Y$ is numerical and $X$ is categorical, there is a very nice way to do this, very easily: Plot a different KDE for each category\n",
    "- This is an extremely powerful way to make expressive visualizations\n",
    "\n",
    "| Method | Usage |\n",
    "| :---: | :---:|\n",
    "| `sns.kdeplot(df[var], hue = cat, common_norm=False)` | Conditional KDE Plot |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3158c7d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conditioning Categorically\n",
    "- We can do the same kind of thing above with statistics instead of graphs\n",
    "- Let `var` be the variable of interest, and `cat` a categorical variable\n",
    "\n",
    "| Method | Usage |\n",
    "| :---: | :---:|\n",
    "| `df.loc[:,[var,cat] ].groupby(cat).mean()` | Groupby calculation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f889a66f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scatter Plot\n",
    "- With the hued KDE, we conditional a numerical on a categorical\n",
    "- If we condition a numerical $Y$ on a numerical $X$, we get: A scatterplot\n",
    "- Precisely, we have a set of points $(x_i, y_i)$, and plot them on the same graph\n",
    "- This gives us a sense of how $Y$ varies with $X$\n",
    "\n",
    "| Method | Usage |\n",
    "| :---: | :---:|\n",
    "| `sns.scatterplot(x=df[var], y = df[var], alpha=.1)` | Scatter plot |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48302c18",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Covariance\n",
    "- What is the linear association between two variables?\n",
    "- We do something like the variance, but interacting the two variables together:\n",
    "$$\n",
    "\\hat{c}_{XY} = \\frac{(x_1-\\bar{x})(y_1-\\bar{y})+ (x_2-\\bar{x})(y_2-\\bar{y})+...+(x_N-\\bar{x})(y_N-\\bar{y})}{N}\n",
    "$$\n",
    "- This only captures **co-linear assocation**: We can find examples of non-linear association where the \"positive\" and \"negative\" parts cancel out to zero, suggesting no association at all (e.g. $y = x^2 + \\varepsilon$, with $x$ between $-1$ and $+1$)\n",
    "- Let `var_list = [var_1, var_2, ...]` be a list of variables for which we want covariances\n",
    "\n",
    "| Method | Usage |\n",
    "| :---: | :---:|\n",
    "| `df.loc[:,var_list].cov(numeric_only=True)` | Covariance Matrix |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd6aef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Correlation\n",
    "- Covariance is useful, but it is hard to interpret across pairs because the units vary\n",
    "- To normalize it, we often look at the correlation instead:\n",
    "$$\n",
    "r_{XY}= \\dfrac{c_{XY}}{s_X s_Y}\n",
    "$$\n",
    "- This number must be between -1 and 1, and it gives a normalized summary of how strongly each $Y$ and $X$ are linearly related\n",
    "\n",
    "| Method | Usage |\n",
    "| :---: | :---:|\n",
    "| `df.loc[:,var_list].corr(numeric_only=True)` | Correlation Matrix |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6dcc57",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contingency Table\n",
    "- We've covered numerical by categorical and numerical by numerical\n",
    "- What about categorical by categorical?\n",
    "- We can make a table with the labels for each of the variables along the side and top, and tabulate the number of co-occurences in the table\n",
    "- This is more interpretable if we normalize, so we can see the proportions of counts in each of the cases\n",
    "\n",
    "| Method | Usage |\n",
    "| :---: | :---:|\n",
    "| `pd.crosstab(df[var_1], df[var_2])` | Raw contingency table |\n",
    "| `pd.crosstab(df[var_1], df[var_2], dropna=True, normalize=True)` | Crosstabulate and normalize |\n",
    "| `pd.crosstab(df[var_1], df[var_2], margins=True, dropna=True, normalize=True)` | Add margins |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
